{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lezione 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "from liblet import Grammar, Derivation, Table, suffixes, closure, warn, HASH, ε"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_εfirst(G):\n",
        "  FIRST = Table(1, element = set)\n",
        "  for t in G.T: FIRST[(t, )] = {t}\n",
        "  FIRST[tuple()] = {ε}\n",
        "  FIRST[(ε, )] = {ε}\n",
        "  FIRST[(HASH, )] = {HASH}\n",
        "  @closure\n",
        "  def update_with_suffixes(FIRST):\n",
        "    for N, α in G.P:\n",
        "      FIRST[(N, )] |= FIRST[α]\n",
        "      for γ in suffixes(α):\n",
        "        A, *β = γ\n",
        "        FIRST[γ] |= FIRST[(A, )] - {ε}\n",
        "        if ε in FIRST[(A, )]: FIRST[γ] |= FIRST[β]\n",
        "    return FIRST\n",
        "  return update_with_suffixes(FIRST)\n",
        "\n",
        "def compute_follow(G, FIRST):\n",
        "  FOLLOW = Table(1, set)\n",
        "  FOLLOW[G.S] |= {HASH}\n",
        "  @closure\n",
        "  def complete_follow(FOLLOW):\n",
        "    for X, ω in G.P:\n",
        "      for γ in suffixes(ω):\n",
        "        A, *β = γ\n",
        "        if A not in G.N: continue\n",
        "        FOLLOW[A] |= FIRST[β] - {ε}\n",
        "        if ε in FIRST[β]: FOLLOW[A] |= FOLLOW[X]\n",
        "    return FOLLOW\n",
        "  return complete_follow(FOLLOW)\n",
        "\n",
        "def compute_table(G, FIRST, FOLLOW):\n",
        "  TABLE = Table(2)\n",
        "  for P in G.P:\n",
        "    A, α = P\n",
        "    for a in FIRST[α] - {'ε'}:\n",
        "        TABLE[A, a] = P\n",
        "    if 'ε' in FIRST[α]:\n",
        "      for a in FOLLOW[A]:\n",
        "          TABLE[A, a] = P\n",
        "  return TABLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simboli e input: stringhe e sequenze\n",
        "\n",
        "Un \"chiarimento\" dovuto sull'uso dei termini *simboli* e *input* e sulle relative scelte implementative adottate per il software prodotto per questo corso.\n",
        "\n",
        "In questi *handout*, i **simboli** (siano essi terminali che non) sono implementati come stringhe, che corrispondono al tipo [Text Sequence Type](https://docs.python.org/3.7/library/stdtypes.html#text-sequence-type-str) (brevemente `str`). Osservate che in Python *non esiste il tipo carattere*, pertanto anche quando i simboli sono composti da un'unica lettera, questi saranno di tipo `str`. \n",
        "\n",
        "Questo è evidente se, ad esempio, si ispeziona una `Grammar`. Prendiamo come esempio quella vista nella scorsa lezione:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>td, th {border: 1pt solid lightgray !important ;}</style><table><tr><th><pre>Expr</pre><td style=\"text-align:left\"><pre>Term Expr′<sub>(0)</sub></pre><tr><th><pre>Expr′</pre><td style=\"text-align:left\"><pre>PLUS Term Expr′<sub>(1)</sub> | ε<sub>(2)</sub></pre><tr><th><pre>Term</pre><td style=\"text-align:left\"><pre>Factor Factor′<sub>(3)</sub></pre><tr><th><pre>Factor′</pre><td style=\"text-align:left\"><pre>TIMES Factor Factor′<sub>(4)</sub> | ε<sub>(5)</sub></pre><tr><th><pre>Factor</pre><td style=\"text-align:left\"><pre>LPAR Expr RPAR<sub>(6)</sub> | NUMBER<sub>(7)</sub></pre></table>"
            ],
            "text/plain": [
              "(Expr -> Term Expr′,\n",
              " Expr′ -> PLUS Term Expr′,\n",
              " Expr′ -> ε,\n",
              " Term -> Factor Factor′,\n",
              " Factor′ -> TIMES Factor Factor′,\n",
              " Factor′ -> ε,\n",
              " Factor -> LPAR Expr RPAR,\n",
              " Factor -> NUMBER)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "G = Grammar.from_string(\"\"\"\n",
        "Expr -> Term Expr′\n",
        "Expr′ -> PLUS Term Expr′ | ε\n",
        "Term -> Factor Factor′\n",
        "Factor′ -> TIMES Factor Factor′ | ε\n",
        "Factor -> LPAR Expr RPAR | NUMBER\n",
        "\"\"\")\n",
        "G.P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Expr', 'Expr′', 'Factor', 'Factor′', 'Term'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sono stringhe (elementi di tipo str) i non terminali\n",
        "\n",
        "set(G.N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'LPAR', 'NUMBER', 'PLUS', 'RPAR', 'TIMES'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# e anche i terminali!\n",
        "\n",
        "set(G.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In base alla definizione di **linguaggio**, le **parole** e le **forme sentenziali** sono sequenze di simboli (sugli opportuni alfabeti). Ragion per cui l'implementazione naturale di tali entità è fatta usando [liste](https://docs.python.org/3.7/tutorial/datastructures.html#more-on-lists) o [tuple](https://docs.python.org/3.7/tutorial/datastructures.html#tuples-and-sequences).\n",
        "\n",
        "Di nuovo, questo è evidente se, ad esempio, si ispeziona una forma sentenziale di una `Derivation`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('NUMBER', 'PLUS', 'Term', 'Expr′')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Derivation(G).leftmost((0, 3, 7, 5, 1)).sentential_form()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In linea di principio, quindi, l'*input* (termine informale) che, dei nostri algoritmi, è costituito da una *parola* (termine formale) deve essere una sequenza di simboli, quindi una lista o tupla.\n",
        "\n",
        "Non ha senso pensare all'*input* come a un elemento di tipo `str`, perché questo impedisce di riconoscere (in modo non \"ambiguo\") i simboli di cui è composto.\n",
        "\n",
        "Prestate attenzione alla diversità tra seguenti asserzioni:\n",
        "* `('NUMBER', 'NUMBER')` è una parola, ma non appartiene al linguaggio generato da `G`; \n",
        "* `'NUMBERNUMBER'` non solo non appartiene a tale linguaggio, ma non è nemmeno una parola sull'alfabeto `G.T` dal momento che, pur essendo un oggetto di tipo `str`, non è una sequenza di simboli terminali (contenuti in `G.T`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Se i simboli sono tutti lunghi uno?\n",
        "\n",
        "Può accadere (ed è accaduto senza che lo rendessi esplicito, cosa che può avervi indotti in confusione), che se i simboli terminali sono **tutti** stringhe di lunghezza uno, l'*input* possa essere \"impropriamente\" implementato, invece che con una lista di stringhe di lunghezza uno, come una stringa. \n",
        "\n",
        "Questa \"confusione\" tra i tipi è resa possibile dal fatto che in Python l'espressione `INPUT[i]` è legittima sia nel caso in cui `INPUT` si riferisca a una stringa, che a una lista o a una tupla; inoltre nel caso in cui i simboli siano stringhe di lunghezza uno, le due espressiono hanno lo stesso valore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# implementazione propria\n",
        "\n",
        "INPUT = ('p', 'i', 'p', 'p', 'o') # o analogamente list('pippo')\n",
        "\n",
        "INPUT[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# implementazione impropria\n",
        "\n",
        "INPUT = 'pippo'\n",
        "\n",
        "INPUT[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizzazione e parsing\n",
        "\n",
        "La confusione si può fare ancora più acuta quando parliamo di *tokenizzazione*, oltre che di parsing.\n",
        "\n",
        "In tal caso ci sono due grammatiche (e quindi due linguaggi), in gioco.\n",
        "\n",
        "La prima grammatica $G_t = (N_t, T_t, P_t, S_t)$ è quella del **tokenizzatore**, è in generale una grammatica regolare i cui terminali $T_t$ sono i caratteri dell'alfabeto di macchina (ad esempio i caratteri *Unicode*). \n",
        "\n",
        "Tale grammatica può essere pensata come l'unione di $k>0$ grammatiche regolari $G^k_t = (N^k_t, T_t, P^k_t, S^k_t)$ (in cui gli $N^k_t$ e $P^k_t$ sono disgiunti, gli $S^k_t$ sono distinti e \n",
        "i linguaggi $L(G^k_t)$ sono disgiunti al variare di $k$), ciascuna delle quali riconosce un certo \"tipo\" di *token*. $G_t$ è usualmente definita da $N_t = \\{S_t\\} \\cup \\bigcup N^k_t$ e $P_t =  \\{S_t \\to ( S^1_t | S^2_t |  \\ldots | S^k_t )^* \\} \\cup \\bigcup P^k_t$ (dove la produzione per il simbolo iniziale, qui descritta con la notazione impropria delle espressioni regolari, indica che le parole $G_t$ sono sequenze di parole delle varie $G^k_t$, ossia di token.\n",
        "\n",
        "La seconda grammatica $G_p = (N_p, T_p, P_p, S_p)$ è quella del **parser**, è in generale una grammatica libera da contesto i cui terminali $T_p$ sono i simboli distinti delle grammatiche $G^k_t$, ossia $T_p = \\{S^1_t, S^2_t, \\ldots, S^k_t\\}$. \n",
        "\n",
        "Rispetto alla situazione in cui si considera solo il parsing, in questo contesto parlare di *input* può condurre ad ancor maggior confusione: tale termine può infatti riferirsi sia alla parola (in $T_t^*$) data in input al tokenizzatore, che alla parola (in $T_p^*$) da esso restituita che sarà quindi data in input al parser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Un esempio concreto\n",
        "\n",
        "Consideriamo come grammatica del parser quella vista nella precedente lezione, nella versione ripropsta all'inizio di questo handout; la grammatica del suo tokenizzatore è definita attraverso le espressioni regolari per `LPAR`, `PLUS`, `RPAR`, `TIMES` (che sono elementari, trattandosi di linguaggi finiti), e quella di `NUMBER` che è semplicemente `\\d+`.\n",
        "\n",
        "Per implementare il tokenizzatore seguiremo l'approccio basato sulle espressioni regolari visto in [L07](L07.ipynb); iniziamo con l'elenco di token e relative espressioni regolari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPXR_TOKEN2PATTERN = (\n",
        "  ('LPAR',   r'\\('),\n",
        "  ('RPAR',   r'\\)'),\n",
        "  ('PLUS',   r'\\+'),\n",
        "  ('TIMES',  r'\\*'),\n",
        "  ('NUMBER', r'\\d+')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Useremo due variabili globali `token` e `value` per immagazzinare il *look-ahead*; data la parola in input al tokenizzatore e l'elenco di espressioni regolari che definiscono la sua grammatica, la seguente funzione "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_consume(word, token2pattern):\n",
        "  tokens_re = re.compile('|'.join(f'(?P<{token}>{pattern})' for token, pattern in token2pattern))\n",
        "  token_value_iterator = iter((token, value) for match in tokens_re.finditer(word) for token, value in match.groupdict().items() if value)\n",
        "  def _advance():\n",
        "    global token, value\n",
        "    token, value = next(token_value_iterator, (HASH, None))\n",
        "  _advance()\n",
        "  def consume(expected):\n",
        "    if token != expected:\n",
        "      warn('Expected {}, found {}'.format(expected, token))\n",
        "    else:\n",
        "      _advance()\n",
        "  return consume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "consente di costruire la funzione `consume` che tenterà di consumare il token atteso e, qualora esso coincida con il look-ahead, avanzerà nella lettura aggiornando i valori di `token` e `value`.\n",
        "\n",
        "Possiamo testarla ad esempio sull'input (`5`, `2`, `*`, `7`) sull'alfabeto Unicode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('NUMBER', '52')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT = '52 * 7'\n",
        "\n",
        "consume = make_consume(INPUT, EPXR_TOKEN2PATTERN)\n",
        "\n",
        "token, value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('TIMES', '*')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "consume('NUMBER')\n",
        "\n",
        "token, value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('NUMBER', '7')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "consume('TIMES')\n",
        "\n",
        "token, value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al parser non resta che determinare la derivazione che produce la sequenza di token visti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Expr -> Term Expr′ -> Factor Factor′ Expr′ -> NUMBER Factor′ Expr′ -> NUMBER TIMES Factor Factor′ Expr′ -> NUMBER TIMES NUMBER Factor′ Expr′ -> NUMBER TIMES NUMBER Expr′ -> NUMBER TIMES NUMBER"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Derivation(G).leftmost((0, 3, 7, 4, 7, 5, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parser ricorsivo discendente predittivo\n",
        "\n",
        "Costruiamo ora un parser ricorsivo discendende (come quello sviluppato in [L09](L09.ipynb)) che usi le informazioni della tabella `TABLE` per decidere quali chiamate ricorsive fare (in funzione delle produzioni contenute nella tabella).\n",
        "\n",
        "In questa prima implementazione non ci occupiano del valore dei token, l'unico obiettivo è determinare se l'espressione è valida.\n",
        "\n",
        "Calcoliamo la `TABLE` che ci servirà per scrivere la procedure ricorsive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>td, th {border: 1pt solid lightgray !important; text-align: left !important;}</style><table><tr><td>&nbsp;<th><pre>NUMBER</pre><th><pre>LPAR</pre><th><pre>PLUS</pre><th><pre>RPAR</pre><th><pre>♯</pre><th><pre>TIMES</pre>\n",
              "<tr><th><pre>Expr<pre><td><pre>Expr -> Term Expr′</pre><td><pre>Expr -> Term Expr′</pre><td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;\n",
              "<tr><th><pre>Expr′<pre><td>&nbsp;<td>&nbsp;<td><pre>Expr′ -> PLUS Term Expr′</pre><td><pre>Expr′ -> ε</pre><td><pre>Expr′ -> ε</pre><td>&nbsp;\n",
              "<tr><th><pre>Term<pre><td><pre>Term -> Factor Factor′</pre><td><pre>Term -> Factor Factor′</pre><td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;\n",
              "<tr><th><pre>Factor′<pre><td>&nbsp;<td>&nbsp;<td><pre>Factor′ -> ε</pre><td><pre>Factor′ -> ε</pre><td><pre>Factor′ -> ε</pre><td><pre>Factor′ -> TIMES Factor Factor′</pre>\n",
              "<tr><th><pre>Factor<pre><td><pre>Factor -> NUMBER</pre><td><pre>Factor -> LPAR Expr RPAR</pre><td>&nbsp;<td>&nbsp;<td>&nbsp;<td>&nbsp;\n",
              "</table>"
            ],
            "text/plain": [
              "<liblet.utils.Table at 0x7fa90815d910>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FIRST = compute_εfirst(G)\n",
        "FOLLOW = compute_follow(G, FIRST)\n",
        "compute_table(G, FIRST, FOLLOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Per ciascun non terminale, tramutiamo la riga relativa in una serie di chiamate ricorsive distinguendo tra le varie colonne grazie al *look-ahead*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_Expr():\n",
        "  if token in {'LPAR', 'NUMBER'}:\n",
        "    parse_Term()\n",
        "    parse_Exprp()\n",
        "  else:\n",
        "    warn('Error parsing Expr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_Exprp():\n",
        "  if token in {'RPAR', HASH}:\n",
        "    pass # questa è l'ε-produzione\n",
        "  elif token == 'PLUS':\n",
        "    consume('PLUS')\n",
        "    parse_Term()\n",
        "    parse_Exprp()\n",
        "  else:\n",
        "    warn('Error parsing Expr′')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_Term():\n",
        "  if token in {'LPAR', 'NUMBER'}:\n",
        "    parse_Factor()\n",
        "    parse_Factorp()\n",
        "  else:\n",
        "    warn('Error parsing Term')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_Factorp():\n",
        "  if token in {'PLUS', 'RPAR', HASH}:\n",
        "    pass\n",
        "  elif token == 'TIMES':\n",
        "    consume('TIMES')\n",
        "    parse_Factor()\n",
        "    parse_Factorp()\n",
        "  else:\n",
        "    warn('Error parsing Factor′')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_Factor():\n",
        "  if token == 'NUMBER':\n",
        "    consume('NUMBER')\n",
        "  elif token == 'LPAR':\n",
        "    consume('LPAR')\n",
        "    parse_Expr()\n",
        "    consume('RPAR')\n",
        "  else:\n",
        "    warn('Error parsing Factor')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Possiamo collaudare il parser su una espressione valida e una contenente un errore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT = '52 * 7'\n",
        "\n",
        "consume = make_consume(INPUT, EPXR_TOKEN2PATTERN)\n",
        "\n",
        "parse_Expr() # non viene restituito nulla, ossia la parola è riconosciuta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error parsing Factor′\n",
            "Error parsing Expr′\n"
          ]
        }
      ],
      "source": [
        "INPUT = '52 7'\n",
        "\n",
        "consume = make_consume(INPUT, EPXR_TOKEN2PATTERN)\n",
        "\n",
        "parse_Expr() # la parola non è una espressione valida, quindi viene restituito un errore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <span style=\"color: red;\">Esercizio</span>\n",
        "\n",
        "Non è difficile scrivere un *generatore di parser* come abbiamo fatto nella lezione [L09](L09.ipynb); similmente si potrebbero modificare le procedure sopra riportate (o il generatore di parser) in modo che restituisca la derivazione ottenuta, sempre seguendo l'esmpio di quanto fatto nella lezione [L09](L13.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## La valutazione di espressioni\n",
        "\n",
        "Il discorso si complica un po' se volessimo ottenere il valore dell'espressione aritmetica, in quel caso l'atomo `NUMBER` andrebbe rimpiazzato da un numero e ciascuna procedura dovrebbe restituire il valore della sottoespressione di cui ha fatto il parsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_Expr():\n",
        "  if token in {'LPAR', 'NUMBER'}:\n",
        "    val = eval_Term()\n",
        "    val += eval_Exprp()\n",
        "    return val\n",
        "  warn('Error parsing Expr')\n",
        "\n",
        "def eval_Exprp():\n",
        "  if token in {'RPAR', HASH}:\n",
        "    return 0 # questa è l'unità addittiva\n",
        "  elif token == 'PLUS':\n",
        "    consume('PLUS')\n",
        "    val = eval_Term()\n",
        "    val += eval_Exprp()\n",
        "    return val\n",
        "  warn('Error parsing Expr′')\n",
        "\n",
        "def eval_Term():\n",
        "  if token in {'LPAR', 'NUMBER'}:\n",
        "    val = eval_Factor()\n",
        "    val *= eval_Factorp()\n",
        "    return val\n",
        "  warn('Error parsing Term')\n",
        "\n",
        "def eval_Factorp():\n",
        "  if token in {'PLUS', 'RPAR', HASH}:\n",
        "    return 1 # questa è l'unità moltiplicativa\n",
        "  elif token == 'TIMES':\n",
        "    consume('TIMES')\n",
        "    val = eval_Factor()\n",
        "    val *= eval_Factorp()\n",
        "    return val\n",
        "  warn('Error parsing Factor′')\n",
        "\n",
        "def eval_Factor():\n",
        "  if token == 'NUMBER':\n",
        "    val = int(value) # la variabile globale modificata da consume\n",
        "    consume('NUMBER')\n",
        "    return val\n",
        "  elif token == 'LPAR':\n",
        "    consume('LPAR')\n",
        "    val = eval_Expr()\n",
        "    consume('RPAR')\n",
        "    return val\n",
        "  warn('Error parsing Factor')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Siamo pronti ad usare queste funzioni per valutare le espressioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT = '2 * 3 + 4'\n",
        "\n",
        "consume = make_consume(INPUT, EPXR_TOKEN2PATTERN)\n",
        "\n",
        "eval_Expr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extended CFG\n",
        "\n",
        "Se con la notazione $\\left\\{\\alpha\\right\\}^*$ indichiamo 0 o più ripetizioni di $\\alpha$, la precedene grammatica si può intendere come\n",
        "\n",
        "$E \\to T \\left\\{+ T\\right\\}^* \\\\\n",
        "T \\to F \\left\\{* F\\right\\}^* \\\\\n",
        "F \\to ( E ) | i$\n",
        "\n",
        "il che suggerisce l'uso di un cicli `while` (invece della ricorsione di coda) per gestire la ripetizione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_Expr():\n",
        "  if token in {'LPAR', 'NUMBER'}:\n",
        "    val = eval_Term()\n",
        "    while token == 'PLUS':\n",
        "      consume('PLUS')\n",
        "      val += eval_Term()\n",
        "    if token in {'RPAR', HASH}: return val\n",
        "  warn('Error parsing Expr')\n",
        "\n",
        "def eval_Term():\n",
        "  if token in {'LPAR', 'NUMBER'}:\n",
        "    val = eval_Factor()\n",
        "    while token == 'TIMES':\n",
        "      consume('TIMES')\n",
        "      val *= eval_Factor()\n",
        "    if token in {'PLUS', 'RPAR', HASH}: return val  \n",
        "  warn('Error parsing Term')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un ultimo test suggerisce che l'approccio funziona!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT = '2 * 3 * 4 + 5 * ( 6 + 7 )'\n",
        "\n",
        "consume = make_consume(INPUT, EPXR_TOKEN2PATTERN)\n",
        "\n",
        "eval_Expr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ma restsa un problemino con `)`, perché? come potrebbe essere risolto?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT = '2 )'\n",
        "\n",
        "consume = make_consume(INPUT, EPXR_TOKEN2PATTERN)\n",
        "\n",
        "eval_Expr()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
